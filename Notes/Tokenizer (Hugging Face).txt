Tokenizer Notes 

Les tokenizers sont l'un des composants cl√©s du pipeline NLP. Leur objectif est de traduire le texte en donn√©es exploitables par le mod√®le.

Dans les t√¢ches de TALN, les donn√©es g√©n√©ralement trait√©es sont du texte brut. Voici un exemple : Jim Henson √©tait un marionnettiste

Cependant, les mod√®les ne peuvent traiter que des nombres ; il faut donc trouver un moyen de convertir le texte brut en nombres. C'est ce que font les tokenizers.

BertTokenizer ‚Äî c‚Äôest quoi ?
C‚Äôest la classe du tokenizer associ√©e au mod√®le BERT.

Son r√¥le est de transformer du texte brut (comme une phrase) en tokens num√©riques que le mod√®le comprend.

Exemple : "This is fake" ‚Üí [101, 2023, 2003, 10948, 102]

.from_pretrained("bert-base-uncased") ‚Äî √ßa veut dire quoi ?
Tu t√©l√©charges automatiquement un tokenizer pr√©entra√Æn√© compatible avec le mod√®le BERT.

"bert-base-uncased" veut dire :

bert ‚Üí mod√®le BERT standard

base ‚Üí 12 couches (taille moyenne)

uncased ‚Üí pas de distinction entre majuscules/minuscules (tout est en minuscules)

Que contient ce tokenizer ?
Un vocabulaire de tokens (environ 30‚ÄØ000)

Un syst√®me de tokenisation sub-word (ex : unbelievable ‚Üí un, ##believable)

Des fichiers de configuration :

vocab.txt

tokenizer_config.json

special_tokens_map.json

tokenizer("This is fake", padding=True, truncation=True, max_length=512)

Ce qui retourne un dictionnaire avec :

python
Copier le code
{
  'input_ids': [...],         # les IDs des tokens
  'attention_mask': [...]     # 1 si le token est utile, 0 si c‚Äôest du padding
}


Trainer n‚Äôutilise que des colonnes num√©riques
Le Trainer de Hugging Face ne peut pas utiliser directement :

du texte (content)

des cha√Ænes de caract√®res

Il s‚Äôattend √† ce que chaque exemple du dataset contienne :

input_ids (entiers)

attention_mask (entiers)

label (entier)


train_dataset.set_format("torch")
üß† Que fait cette ligne ?
‚úÖ Elle convertit ton dataset Hugging Face en format compatible avec PyTorch
Concr√®tement :

Avant, tes donn√©es sont stock√©es dans un format interne (Arrow table)

Apr√®s .set_format("torch"), chaque √©l√©ment (comme input_ids, label, etc.) devient un torch.Tensor

Et √ßa, c‚Äôest exactement ce que BERT (et le Trainer) attend comme entr√©e.

üß™ Exemple :
Avant set_format("torch"), un exemple de train_dataset[0] peut √™tre :

python
Copier le code
{
  'input_ids': [101, 2023, 2003, 1037, 10948, 2739, 1012, 102],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1],
  'label': 0
}
Apr√®s set_format("torch") :

python
Copier le code
{
  'input_ids': tensor([101, 2023, 2003, 1037, 10948, 2739, 1012, 102]),
  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1]),
  'label': tensor(0)
}
üí° Pourquoi c‚Äôest obligatoire ?
Parce que BERT (via PyTorch) s‚Äôattend √† ce que tout soit en torch.Tensor.

BertForSequenceClassification
C‚Äôest un mod√®le BERT pr√©-entra√Æn√©, sp√©cialement con√ßu pour la classification de s√©quences.


Tu dois comprendre quelques bases minimales pour :
Cr√©er un tensor

Passer des donn√©es dans un mod√®le avec model(input_ids=..., attention_mask=...)

Lire les pr√©dictions avec torch.argmax(...)

D√©sactiver le mode entra√Ænement avec model.eval()

Utiliser with torch.no_grad() pour pr√©dire sans calcul inutile

üß† √âtape 2 ‚Äî Cr√©er un script de pr√©diction
Un script predict.py qui :

Charge le mod√®le et le tokenizer,

Prend un fichier .txt (ou .csv) en entr√©e,

Retourne :

la classe pr√©dite (Fake ou Real),

le score de confiance (softmax du mod√®le).